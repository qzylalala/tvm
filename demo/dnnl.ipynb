{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:11:23] /home/qzylalala/work_space/tvm/src/runtime/logging.cc:307: TVM_LOG_DEBUG enables VLOG statements in 'ir/transform.cc' up to level 1\n",
      "[12:11:23] /home/qzylalala/work_space/tvm/src/runtime/logging.cc:307: TVM_LOG_DEBUG enables VLOG statements in 'relay/ir/transform.cc' up to level 1\n",
      "/home/qzylalala/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:11:24] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: InferType: Executing module pass with opt level: 0\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "\n",
    "\n",
    "def get_demo_mod():\n",
    "    d1 = relay.var(\"d1\", shape=(1, 32, 56, 56), dtype=\"float32\")\n",
    "    w1 = relay.var(\"w1\", shape=(32, 32, 3, 3), dtype=\"float32\")\n",
    "    b1 = relay.var(\"b1\", shape=(32,), dtype=\"float32\")\n",
    "    conv = relay.nn.conv2d(d1, w1, strides=(1, 1), padding=(1, 1))\n",
    "    bias = relay.nn.bias_add(conv, b1)\n",
    "    relu = relay.nn.relu(bias)\n",
    "\n",
    "    func = relay.Function([d1, w1, b1], relu)\n",
    "    mod = tvm.IRModule.from_expr(func)\n",
    "    mod = relay.transform.InferType()(mod)\n",
    "    return mod\n",
    "\n",
    "\n",
    "mod = get_demo_mod()\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass RemoveUnusedFunctions\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: RemoveUnusedFunctions: Executing module pass with opt level: 1\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass ToBasicBlockNormalForm\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: ToBasicBlockNormalForm: Executing module pass with opt level: 1\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass qnn.Legalize\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass QnnLegalize\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: QnnLegalize: Executing function pass with opt level: 1\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: QnnLegalize: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: QnnLegalize: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: QnnLegalize: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass QnnCanonicalize\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: QnnCanonicalize: Executing function pass with opt level: 1\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: QnnCanonicalize: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: QnnCanonicalize: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: QnnCanonicalize: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass Legalize\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: Legalize: Executing function pass with opt level: 1\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: Legalize: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: Legalize: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: Legalize: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass SimplifyInference\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: SimplifyInference: Executing function pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: SimplifyInference: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: SimplifyInference: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyInference: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass EliminateCommonSubexpr\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'EliminateCommonSubexpr'\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass CombineParallelConv2d\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'CombineParallelConv2d'\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass CombineParallelDense\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'CombineParallelDense'\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass CombineParallelBatchMatmul\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'CombineParallelBatchMatmul'\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FoldConstant\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: FoldConstant: Executing function pass with opt level: 2\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: FoldConstant: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: FoldConstant: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: FoldConstant: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FoldScaleAxis\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass BackwardFoldScaleAxis\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'BackwardFoldScaleAxis'\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass ForwardFoldScaleAxis\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'ForwardFoldScaleAxis'\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FoldConstant\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: FoldConstant: Executing function pass with opt level: 2\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: FoldConstant: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: FoldConstant: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: FoldConstant: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass SimplifyExpr\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: SimplifyExpr: Executing function pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: SimplifyExpr: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: SimplifyExpr: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass CanonicalizeCast\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'CanonicalizeCast'\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass CanonicalizeOps\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'CanonicalizeOps'\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FlattenAtrousConv\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: FlattenAtrousConv: Executing function pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: FlattenAtrousConv: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: FlattenAtrousConv: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: FlattenAtrousConv: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass InferType\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass AlterOpLayout\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'AlterOpLayout'\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass SimplifyExprPostAlterOp\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: SimplifyExprPostAlterOp: Executing function pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: SimplifyExprPostAlterOp: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: SimplifyExprPostAlterOp: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FastMath\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'FastMath'\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FoldConstant\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: FoldConstant: Executing function pass with opt level: 2\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: FoldConstant: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: FoldConstant: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: FoldConstant: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass SplitArgs\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: SplitArgs: Executing function pass with opt level: 1\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: SplitArgs: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: SplitArgs: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0]) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SplitArgs: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass PlanDevices\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass PlanDevicesRewrite\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: PlanDevicesRewrite: Executing function pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: PlanDevicesRewrite: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0]) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: PlanDevicesRewrite: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0]) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: PlanDevicesRewrite: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass PlanDevicesCore\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: PlanDevicesCore: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FuseOps\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: FuseOps: Executing function pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: FuseOps: Input module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: FuseOps: Output module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32], %p1: Tensor[(32, 32, 3, 3), float32], %p2: Tensor[(32), float32], Primitive=1) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    %0 = nn.conv2d(%p0, %p1, padding=[1, 1, 1, 1]);\n",
      "    %1 = nn.bias_add(%0, %p2);\n",
      "    nn.relu(%1)\n",
      "  };\n",
      "  %2(%d1, %w1, %b1)\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: FuseOps: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InlineGlobals: Executing module pass with opt level: 1\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: LabelOps: Executing function pass with opt level: 1\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: LabelOps: Input module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %p2: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    %0 = nn.conv2d(%p0, %p1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    %1 = nn.bias_add(%0, %p2) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2(%d1, %w1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: LabelOps: Output module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"069c70b12a6e6bb7\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %p2: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1, hash=\"0f0997370d7cdf0f\", kernel_layout=\"OIHW\", data_layout=\"NCHW\", out_layout=\"\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    %0 = nn.conv2d(%p0, %p1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    %1 = nn.bias_add(%0, %p2) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2(%d1, %w1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: LabelOps: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: AnnotateMemoryScope: Executing function pass with opt level: 2\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: AnnotateMemoryScope: Input module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"069c70b12a6e6bb7\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %p2: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1, hash=\"0f0997370d7cdf0f\", kernel_layout=\"OIHW\", data_layout=\"NCHW\", out_layout=\"\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    %0 = nn.conv2d(%p0, %p1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    %1 = nn.bias_add(%0, %p2) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2(%d1, %w1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: AnnotateMemoryScope: Output module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"069c70b12a6e6bb7\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %p2: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1, hash=\"0f0997370d7cdf0f\", kernel_layout=\"OIHW\", data_layout=\"NCHW\", out_layout=\"\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    %0 = nn.conv2d(%p0, %p1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    %1 = nn.bias_add(%0, %p2) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2(%d1, %w1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: AnnotateMemoryScope: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: Running pass RelayToTIRTargetHook\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: GraphExecutorCodegen: RelayToTIRTargetHook: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: Running pass LowerTE\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: GraphExecutorCodegen: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: GraphExecutorCodegen: LowerTE: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Executing function pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Input module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"069c70b12a6e6bb7\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %p2: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1, hash=\"0f0997370d7cdf0f\", kernel_layout=\"OIHW\", data_layout=\"NCHW\", out_layout=\"\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    %0 = nn.conv2d(%p0, %p1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    %1 = nn.bias_add(%0, %p2) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2(%d1, %w1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'constant_memory_pools' = (nullptr)\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'main_func_info' = FunctionInfoNode(\n",
      "workspace_sizes={c -keys=cpu : 0},\n",
      "  io_sizes={c -keys=cpu : 839808},\n",
      "  constant_sizes={c -keys=cpu : 0},\n",
      "  tir_primfuncs={},\n",
      "  relay_primfuncs={c -keys=cpu : fn (%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"069c70b12a6e6bb7\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %p2: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1, hash=\"0f0997370d7cdf0f\", kernel_layout=\"OIHW\", data_layout=\"NCHW\", out_layout=\"\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    %0 = nn.conv2d(%p0, %p1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    %1 = nn.bias_add(%0, %p2) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2(%d1, %w1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "} /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */\n",
      "})\n",
      "  'runtime' = cpp\n",
      "  'workspace_memory_pools' = (nullptr)\n",
      "}\n",
      "\n",
      "\n",
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectPrefetch\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.TextureFlatten\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlatten\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferShapeLegalize\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferStrideLegalize\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ThreadScopePropagate\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferBindUnwrapper\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ApplyLayoutTransforms\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlattener\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.AssertSimplifier\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerCrossThreadReduction\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerInitBlock\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.PlanAndUpdateBufferAllocationLocation\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ConvertBlocksToOpaque\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnifyThreadBinding\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ManifestSharedMemoryLocalStage\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CompactBufferAllocation\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerAutoCopy\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerMatchBuffer\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectSoftwarePipeline\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerOpaqueBlock\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.FlattenBuffer\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BF16ComputeLegalize\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.NarrowDataType\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LoopPartition\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.VectorizeLoop\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectVirtualThread\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectDoubleBuffer\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageRewrite\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnrollLoop\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RenormalizeSplitPattern\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RewriteUnsafeSelect\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.HoistIfThenElse\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InsertHoistIfThenElse\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CommonSubexprElimTIR\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Output module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"069c70b12a6e6bb7\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = (%d1, %w1, %b1);\n",
      "  call_lowered(@tvmgen_default_fused_nn_conv2d_nn_bias_add_nn_relu, %0, metadata={\"relay_attrs\"={__dict__={\"Primitive\"=1, \"hash\"=\"0f0997370d7cdf0f\", \"kernel_layout\"=\"OIHW\", \"data_layout\"=\"NCHW\", \"out_layout\"=\"\"}}, \"all_prim_fn_vars\"=['tvmgen_default_fused_nn_conv2d_nn_bias_add_nn_relu']})\n",
      "}\n",
      "attributes {\n",
      "  'constant_memory_pools' = (nullptr)\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'main_func_info' = FunctionInfoNode(\n",
      "workspace_sizes={c -keys=cpu : 0},\n",
      "  io_sizes={c -keys=cpu : 839808},\n",
      "  constant_sizes={c -keys=cpu : 0},\n",
      "  tir_primfuncs={},\n",
      "  relay_primfuncs={c -keys=cpu : fn (%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"069c70b12a6e6bb7\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=3e6c2b0, kind='c', keys={'cpu'}, host=Target(id=3ebeb90, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %p2: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1, hash=\"0f0997370d7cdf0f\", kernel_layout=\"OIHW\", data_layout=\"NCHW\", out_layout=\"\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    %0 = nn.conv2d(%p0, %p1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    %1 = nn.bias_add(%0, %p2) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "    nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2(%d1, %w1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "} /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */\n",
      "})\n",
      "  'runtime' = cpp\n",
      "  'workspace_memory_pools' = (nullptr)\n",
      "}\n",
      "\n",
      "\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: Running pass InferType\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: GraphExecutorCodegen: InferType: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: Running pass tir.ExtractPrimFuncConstants\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: GraphExecutorCodegen: tir.ExtractPrimFuncConstants: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.calculate_allocated_bytes\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: tir.calculate_allocated_bytes: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerVtcmAlloc\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.BindTarget\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.VerifyMemory\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: tir.VerifyMemory: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.AnnotateEntryFunc\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.ThreadSync\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.ThreadSync\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.MergeDynamicSharedMemoryAllocations\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.ThreadSync\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.InferFragment\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerThreadAllreduce\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.MakePackedAPI\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: tir.MakePackedAPI: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.BF16StorageLegalize\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.SplitHostDevice\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: tir.SplitHostDevice: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: tir.SplitHostDevice: tir.ConvertSSA: Executing module pass with opt level: 0\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.Filter\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.BindTarget\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerTVMBuiltin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"d1\", \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"w1\", \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"null\", \n",
      "      \"name\": \"b1\", \n",
      "      \"inputs\": []\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"tvm_op\", \n",
      "      \"name\": \"tvmgen_default_fused_nn_conv2d_nn_bias_add_nn_relu\", \n",
      "      \"attrs\": {\n",
      "        \"num_outputs\": \"1\", \n",
      "        \"num_inputs\": \"3\", \n",
      "        \"flatten_data\": \"0\", \n",
      "        \"func_name\": \"tvmgen_default_fused_nn_conv2d_nn_bias_add_nn_relu\", \n",
      "        \"out_layout\": \"\", \n",
      "        \"data_layout\": \"NCHW\", \n",
      "        \"kernel_layout\": \"OIHW\", \n",
      "        \"hash\": \"0f0997370d7cdf0f\"\n",
      "      }, \n",
      "      \"inputs\": [\n",
      "        [\n",
      "          0, \n",
      "          0, \n",
      "          0\n",
      "        ], \n",
      "        [\n",
      "          1, \n",
      "          0, \n",
      "          0\n",
      "        ], \n",
      "        [\n",
      "          2, \n",
      "          0, \n",
      "          0\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  ], \n",
      "  \"arg_nodes\": [0, 1, 2], \n",
      "  \"heads\": [\n",
      "    [\n",
      "      3, \n",
      "      0, \n",
      "      0\n",
      "    ]\n",
      "  ], \n",
      "  \"attrs\": {\n",
      "    \"dltype\": [\n",
      "      \"list_str\", \n",
      "      [\n",
      "        \"float32\", \n",
      "        \"float32\", \n",
      "        \"float32\", \n",
      "        \"float32\"\n",
      "      ]\n",
      "    ], \n",
      "    \"device_index\": [\n",
      "      \"list_int\", \n",
      "      [1, 1, 1, 1]\n",
      "    ], \n",
      "    \"storage_id\": [\n",
      "      \"list_int\", \n",
      "      [0, 1, 2, 3]\n",
      "    ], \n",
      "    \"shape\": [\n",
      "      \"list_shape\", \n",
      "      [\n",
      "        [1, 32, 56, 56], \n",
      "        [32, 32, 3, 3], \n",
      "        [32], \n",
      "        [1, 32, 56, 56]\n",
      "      ]\n",
      "    ]\n",
      "  }, \n",
      "  \"node_row_ptr\": [0, 1, 2, 3, 4]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerCustomDatatypes\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerIntrin\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerDeviceStorageAccessInfo\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.CombineContextCall\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.Filter\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.BindTarget\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerWarpMemory\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.Simplify\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerCustomDatatypes\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerDeviceStorageAccessInfo\n",
      "[12:11:36] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerIntrin\n",
      "/tmp/ipykernel_62638/48896817.py:2: DeprecationWarning: legacy graph executor behavior of producing json / lib / params will be removed in the next release. Please see documents of tvm.contrib.graph_executor.GraphModule for the  new recommended usage.\n",
      "  graph, lib, params = relay.build(mod, target=\"c\", params=None)\n"
     ]
    }
   ],
   "source": [
    "with tvm.transform.PassContext(opt_level=2):\n",
    "    graph, lib, params = relay.build(mod, target=\"c\", params=None)\n",
    "\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// tvm target: c -keys=cpu \n",
      "#define TVM_EXPORTS\n",
      "#include \"tvm/runtime/c_runtime_api.h\"\n",
      "#include \"tvm/runtime/c_backend_api.h\"\n",
      "#include <math.h>\n",
      "#include <stdbool.h>\n",
      "#ifdef __cplusplus\n",
      "extern \"C\"\n",
      "#endif\n",
      "TVM_DLL int32_t tvmgen_default_fused_nn_conv2d_nn_bias_add_nn_relu(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n",
      "  void* arg_p0 = (((TVMValue*)args)[0].v_handle);\n",
      "  int32_t arg_p0_code = arg_type_ids[0];\n",
      "  void* arg_p1 = (((TVMValue*)args)[1].v_handle);\n",
      "  int32_t arg_p1_code = arg_type_ids[1];\n",
      "  void* arg_p2 = (((TVMValue*)args)[2].v_handle);\n",
      "  int32_t arg_p2_code = arg_type_ids[2];\n",
      "  void* arg_T_relu = (((TVMValue*)args)[3].v_handle);\n",
      "  int32_t arg_T_relu_code = arg_type_ids[3];\n",
      "  void* p0 = (((DLTensor*)arg_p0)[0].data);\n",
      "  void* arg_p0_shape = (((DLTensor*)arg_p0)[0].shape);\n",
      "  void* arg_p0_strides = (((DLTensor*)arg_p0)[0].strides);\n",
      "  int32_t dev_id = (((DLTensor*)arg_p0)[0].device.device_id);\n",
      "  void* p1 = (((DLTensor*)arg_p1)[0].data);\n",
      "  void* arg_p1_shape = (((DLTensor*)arg_p1)[0].shape);\n",
      "  void* arg_p1_strides = (((DLTensor*)arg_p1)[0].strides);\n",
      "  void* p2 = (((DLTensor*)arg_p2)[0].data);\n",
      "  void* arg_p2_shape = (((DLTensor*)arg_p2)[0].shape);\n",
      "  void* arg_p2_strides = (((DLTensor*)arg_p2)[0].strides);\n",
      "  void* T_relu = (((DLTensor*)arg_T_relu)[0].data);\n",
      "  void* arg_T_relu_shape = (((DLTensor*)arg_T_relu)[0].shape);\n",
      "  void* arg_T_relu_strides = (((DLTensor*)arg_T_relu)[0].strides);\n",
      "  if (!(arg_p0_strides == NULL)) {\n",
      "  }\n",
      "  if (!(arg_p1_strides == NULL)) {\n",
      "  }\n",
      "  if (!(arg_p2_strides == NULL)) {\n",
      "  }\n",
      "  if (!(arg_T_relu_strides == NULL)) {\n",
      "  }\n",
      "  void* data_vec = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)401408, 2, 32);\n",
      "  if (data_vec == NULL) {\n",
      "    return -1;\n",
      "  }\n",
      "  void* data_pad = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)430592, 2, 32);\n",
      "  if (data_pad == NULL) {\n",
      "    return -1;\n",
      "  }\n",
      "  for (int32_t bs_c_fused_h_fused = 0; bs_c_fused_h_fused < 448; ++bs_c_fused_h_fused) {\n",
      "    for (int32_t w = 0; w < 56; ++w) {\n",
      "      for (int32_t vc = 0; vc < 4; ++vc) {\n",
      "        ((float*)data_vec)[(((bs_c_fused_h_fused * 224) + (w * 4)) + vc)] = ((float*)p0)[(((((bs_c_fused_h_fused / 56) * 12544) + (vc * 3136)) + ((bs_c_fused_h_fused % 56) * 56)) + w)];\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 464; ++i0_i1_fused_i2_fused) {\n",
      "    for (int32_t i3 = 0; i3 < 58; ++i3) {\n",
      "      int32_t cse_var_2 = (i0_i1_fused_i2_fused % 58);\n",
      "      int32_t cse_var_1 = (i3 * 4);\n",
      "      int32_t4 v_ = int32_t4(((((((i0_i1_fused_i2_fused / 58) * 12544) + (cse_var_2 * 224)) + cse_var_1) - 228))+(1*0), ((((((i0_i1_fused_i2_fused / 58) * 12544) + (cse_var_2 * 224)) + cse_var_1) - 228))+(1*1), ((((((i0_i1_fused_i2_fused / 58) * 12544) + (cse_var_2 * 224)) + cse_var_1) - 228))+(1*2), ((((((i0_i1_fused_i2_fused / 58) * 12544) + (cse_var_2 * 224)) + cse_var_1) - 228))+(1*3));\n",
      "      *(float4*)(((float*)data_pad) + ((i0_i1_fused_i2_fused * 232) + cse_var_1)) = (((((1 <= cse_var_2) && (cse_var_2 < 57)) && (1 <= i3)) && (i3 < 57)) ? (float4(((float*)data_vec)[v_.s0],((float*)data_vec)[v_.s1],((float*)data_vec)[v_.s2],((float*)data_vec)[v_.s3])) : ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f)));\n",
      "    }\n",
      "  }\n",
      "  for (int32_t occ_k_h_fused = 0; occ_k_h_fused < 24; ++occ_k_h_fused) {\n",
      "    for (int32_t icc = 0; icc < 8; ++icc) {\n",
      "      for (int32_t k_w = 0; k_w < 3; ++k_w) {\n",
      "        for (int32_t icb = 0; icb < 4; ++icb) {\n",
      "          int32_t cse_var_4 = (occ_k_h_fused % 3);\n",
      "          int32_t cse_var_3 = ((occ_k_h_fused / 3) * 1152);\n",
      "          int32_t4 v__1 = int32_t4((((((cse_var_3 + (icc * 36)) + (icb * 9)) + (cse_var_4 * 3)) + k_w))+(288*0), (((((cse_var_3 + (icc * 36)) + (icb * 9)) + (cse_var_4 * 3)) + k_w))+(288*1), (((((cse_var_3 + (icc * 36)) + (icb * 9)) + (cse_var_4 * 3)) + k_w))+(288*2), (((((cse_var_3 + (icc * 36)) + (icb * 9)) + (cse_var_4 * 3)) + k_w))+(288*3));\n",
      "          *(float4*)(((float*)data_vec) + ((((cse_var_3 + (icc * 144)) + (cse_var_4 * 48)) + (k_w * 16)) + (icb * 4))) = (float4(((float*)p1)[v__1.s0],((float*)p1)[v__1.s1],((float*)p1)[v__1.s2],((float*)p1)[v__1.s3]));\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  for (int32_t ax0_ax1_outer_fused_ax2_fused = 0; ax0_ax1_outer_fused_ax2_fused < 448; ++ax0_ax1_outer_fused_ax2_fused) {\n",
      "    float4 conv2d_NCHWc[56];\n",
      "    float4 conv2d_NCHWc_global[28];\n",
      "    for (int32_t ow_outer = 0; ow_outer < 2; ++ow_outer) {\n",
      "      conv2d_NCHWc_global[0] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[1] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[2] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[3] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[4] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[5] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[6] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[7] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[8] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[9] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[10] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[11] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[12] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[13] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[14] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[15] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[16] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[17] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[18] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[19] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[20] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[21] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[22] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[23] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[24] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[25] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[26] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      conv2d_NCHWc_global[27] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      for (int32_t ic_outer = 0; ic_outer < 8; ++ic_outer) {\n",
      "        for (int32_t kh = 0; kh < 3; ++kh) {\n",
      "          for (int32_t kw = 0; kw < 3; ++kw) {\n",
      "            for (int32_t ic_inner = 0; ic_inner < 4; ++ic_inner) {\n",
      "              int32_t cse_var_6 = ((((((ax0_ax1_outer_fused_ax2_fused / 56) * 1152) + (ic_outer * 144)) + (kh * 48)) + (kw * 16)) + (ic_inner * 4));\n",
      "              int32_t cse_var_5 = ((((((ic_outer * 13456) + (kh * 232)) + ((ax0_ax1_outer_fused_ax2_fused % 56) * 232)) + (ow_outer * 112)) + (kw * 4)) + ic_inner);\n",
      "              int32_t4 v__2 = int32_t4((cse_var_6)+(1*0), (cse_var_6)+(1*1), (cse_var_6)+(1*2), (cse_var_6)+(1*3));\n",
      "              conv2d_NCHWc_global[0] = (conv2d_NCHWc_global[0] + (((float4)(((float*)data_pad)[cse_var_5], ((float*)data_pad)[cse_var_5], ((float*)data_pad)[cse_var_5], ((float*)data_pad)[cse_var_5])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[1] = (conv2d_NCHWc_global[1] + (((float4)(((float*)data_pad)[(cse_var_5 + 4)], ((float*)data_pad)[(cse_var_5 + 4)], ((float*)data_pad)[(cse_var_5 + 4)], ((float*)data_pad)[(cse_var_5 + 4)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[2] = (conv2d_NCHWc_global[2] + (((float4)(((float*)data_pad)[(cse_var_5 + 8)], ((float*)data_pad)[(cse_var_5 + 8)], ((float*)data_pad)[(cse_var_5 + 8)], ((float*)data_pad)[(cse_var_5 + 8)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[3] = (conv2d_NCHWc_global[3] + (((float4)(((float*)data_pad)[(cse_var_5 + 12)], ((float*)data_pad)[(cse_var_5 + 12)], ((float*)data_pad)[(cse_var_5 + 12)], ((float*)data_pad)[(cse_var_5 + 12)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[4] = (conv2d_NCHWc_global[4] + (((float4)(((float*)data_pad)[(cse_var_5 + 16)], ((float*)data_pad)[(cse_var_5 + 16)], ((float*)data_pad)[(cse_var_5 + 16)], ((float*)data_pad)[(cse_var_5 + 16)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[5] = (conv2d_NCHWc_global[5] + (((float4)(((float*)data_pad)[(cse_var_5 + 20)], ((float*)data_pad)[(cse_var_5 + 20)], ((float*)data_pad)[(cse_var_5 + 20)], ((float*)data_pad)[(cse_var_5 + 20)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[6] = (conv2d_NCHWc_global[6] + (((float4)(((float*)data_pad)[(cse_var_5 + 24)], ((float*)data_pad)[(cse_var_5 + 24)], ((float*)data_pad)[(cse_var_5 + 24)], ((float*)data_pad)[(cse_var_5 + 24)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[7] = (conv2d_NCHWc_global[7] + (((float4)(((float*)data_pad)[(cse_var_5 + 28)], ((float*)data_pad)[(cse_var_5 + 28)], ((float*)data_pad)[(cse_var_5 + 28)], ((float*)data_pad)[(cse_var_5 + 28)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[8] = (conv2d_NCHWc_global[8] + (((float4)(((float*)data_pad)[(cse_var_5 + 32)], ((float*)data_pad)[(cse_var_5 + 32)], ((float*)data_pad)[(cse_var_5 + 32)], ((float*)data_pad)[(cse_var_5 + 32)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[9] = (conv2d_NCHWc_global[9] + (((float4)(((float*)data_pad)[(cse_var_5 + 36)], ((float*)data_pad)[(cse_var_5 + 36)], ((float*)data_pad)[(cse_var_5 + 36)], ((float*)data_pad)[(cse_var_5 + 36)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[10] = (conv2d_NCHWc_global[10] + (((float4)(((float*)data_pad)[(cse_var_5 + 40)], ((float*)data_pad)[(cse_var_5 + 40)], ((float*)data_pad)[(cse_var_5 + 40)], ((float*)data_pad)[(cse_var_5 + 40)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[11] = (conv2d_NCHWc_global[11] + (((float4)(((float*)data_pad)[(cse_var_5 + 44)], ((float*)data_pad)[(cse_var_5 + 44)], ((float*)data_pad)[(cse_var_5 + 44)], ((float*)data_pad)[(cse_var_5 + 44)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[12] = (conv2d_NCHWc_global[12] + (((float4)(((float*)data_pad)[(cse_var_5 + 48)], ((float*)data_pad)[(cse_var_5 + 48)], ((float*)data_pad)[(cse_var_5 + 48)], ((float*)data_pad)[(cse_var_5 + 48)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[13] = (conv2d_NCHWc_global[13] + (((float4)(((float*)data_pad)[(cse_var_5 + 52)], ((float*)data_pad)[(cse_var_5 + 52)], ((float*)data_pad)[(cse_var_5 + 52)], ((float*)data_pad)[(cse_var_5 + 52)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[14] = (conv2d_NCHWc_global[14] + (((float4)(((float*)data_pad)[(cse_var_5 + 56)], ((float*)data_pad)[(cse_var_5 + 56)], ((float*)data_pad)[(cse_var_5 + 56)], ((float*)data_pad)[(cse_var_5 + 56)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[15] = (conv2d_NCHWc_global[15] + (((float4)(((float*)data_pad)[(cse_var_5 + 60)], ((float*)data_pad)[(cse_var_5 + 60)], ((float*)data_pad)[(cse_var_5 + 60)], ((float*)data_pad)[(cse_var_5 + 60)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[16] = (conv2d_NCHWc_global[16] + (((float4)(((float*)data_pad)[(cse_var_5 + 64)], ((float*)data_pad)[(cse_var_5 + 64)], ((float*)data_pad)[(cse_var_5 + 64)], ((float*)data_pad)[(cse_var_5 + 64)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[17] = (conv2d_NCHWc_global[17] + (((float4)(((float*)data_pad)[(cse_var_5 + 68)], ((float*)data_pad)[(cse_var_5 + 68)], ((float*)data_pad)[(cse_var_5 + 68)], ((float*)data_pad)[(cse_var_5 + 68)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[18] = (conv2d_NCHWc_global[18] + (((float4)(((float*)data_pad)[(cse_var_5 + 72)], ((float*)data_pad)[(cse_var_5 + 72)], ((float*)data_pad)[(cse_var_5 + 72)], ((float*)data_pad)[(cse_var_5 + 72)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[19] = (conv2d_NCHWc_global[19] + (((float4)(((float*)data_pad)[(cse_var_5 + 76)], ((float*)data_pad)[(cse_var_5 + 76)], ((float*)data_pad)[(cse_var_5 + 76)], ((float*)data_pad)[(cse_var_5 + 76)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[20] = (conv2d_NCHWc_global[20] + (((float4)(((float*)data_pad)[(cse_var_5 + 80)], ((float*)data_pad)[(cse_var_5 + 80)], ((float*)data_pad)[(cse_var_5 + 80)], ((float*)data_pad)[(cse_var_5 + 80)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[21] = (conv2d_NCHWc_global[21] + (((float4)(((float*)data_pad)[(cse_var_5 + 84)], ((float*)data_pad)[(cse_var_5 + 84)], ((float*)data_pad)[(cse_var_5 + 84)], ((float*)data_pad)[(cse_var_5 + 84)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[22] = (conv2d_NCHWc_global[22] + (((float4)(((float*)data_pad)[(cse_var_5 + 88)], ((float*)data_pad)[(cse_var_5 + 88)], ((float*)data_pad)[(cse_var_5 + 88)], ((float*)data_pad)[(cse_var_5 + 88)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[23] = (conv2d_NCHWc_global[23] + (((float4)(((float*)data_pad)[(cse_var_5 + 92)], ((float*)data_pad)[(cse_var_5 + 92)], ((float*)data_pad)[(cse_var_5 + 92)], ((float*)data_pad)[(cse_var_5 + 92)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[24] = (conv2d_NCHWc_global[24] + (((float4)(((float*)data_pad)[(cse_var_5 + 96)], ((float*)data_pad)[(cse_var_5 + 96)], ((float*)data_pad)[(cse_var_5 + 96)], ((float*)data_pad)[(cse_var_5 + 96)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[25] = (conv2d_NCHWc_global[25] + (((float4)(((float*)data_pad)[(cse_var_5 + 100)], ((float*)data_pad)[(cse_var_5 + 100)], ((float*)data_pad)[(cse_var_5 + 100)], ((float*)data_pad)[(cse_var_5 + 100)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[26] = (conv2d_NCHWc_global[26] + (((float4)(((float*)data_pad)[(cse_var_5 + 104)], ((float*)data_pad)[(cse_var_5 + 104)], ((float*)data_pad)[(cse_var_5 + 104)], ((float*)data_pad)[(cse_var_5 + 104)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "              conv2d_NCHWc_global[27] = (conv2d_NCHWc_global[27] + (((float4)(((float*)data_pad)[(cse_var_5 + 108)], ((float*)data_pad)[(cse_var_5 + 108)], ((float*)data_pad)[(cse_var_5 + 108)], ((float*)data_pad)[(cse_var_5 + 108)])) * (float4(((float*)data_vec)[v__2.s0],((float*)data_vec)[v__2.s1],((float*)data_vec)[v__2.s2],((float*)data_vec)[v__2.s3]))));\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      for (int32_t ow_inner = 0; ow_inner < 28; ++ow_inner) {\n",
      "        conv2d_NCHWc[((ow_outer * 28) + ow_inner)] = conv2d_NCHWc_global[ow_inner];\n",
      "      }\n",
      "    }\n",
      "    for (int32_t ax3_outer = 0; ax3_outer < 2; ++ax3_outer) {\n",
      "      for (int32_t ax3_inner = 0; ax3_inner < 28; ++ax3_inner) {\n",
      "        int32_t cse_var_8 = (ax0_ax1_outer_fused_ax2_fused / 56);\n",
      "        int32_t cse_var_7 = (ax3_outer * 28);\n",
      "          int32_t4 v__3 = int32_t4((((((cse_var_8 * 12544) + ((ax0_ax1_outer_fused_ax2_fused % 56) * 56)) + cse_var_7) + ax3_inner))+(3136*0), (((((cse_var_8 * 12544) + ((ax0_ax1_outer_fused_ax2_fused % 56) * 56)) + cse_var_7) + ax3_inner))+(3136*1), (((((cse_var_8 * 12544) + ((ax0_ax1_outer_fused_ax2_fused % 56) * 56)) + cse_var_7) + ax3_inner))+(3136*2), (((((cse_var_8 * 12544) + ((ax0_ax1_outer_fused_ax2_fused % 56) * 56)) + cse_var_7) + ax3_inner))+(3136*3));\n",
      "          float4 v__4 = conv2d_NCHWc[(cse_var_7 + ax3_inner)] + *(float4*)(((float*)p2) + (cse_var_8 * 4));\n",
      "          float4 v__5 = (float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f);\n",
      "          float4 v__6 = (v__4) > (v__5) ? (v__4) : (v__5);\n",
      "          ((float*)T_relu)[v__3.s0] = v__6.s0;\n",
      "          ((float*)T_relu)[v__3.s1] = v__6.s1;\n",
      "          ((float*)T_relu)[v__3.s2] = v__6.s2;\n",
      "          ((float*)T_relu)[v__3.s3] = v__6.s3;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  if (TVMBackendFreeWorkspace(1, dev_id, data_pad) != 0) {\n",
      "    return -1;\n",
      "  }\n",
      "  if (TVMBackendFreeWorkspace(1, dev_id, data_vec) != 0) {\n",
      "    return -1;\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "// CodegenC: NOTE: Auto-generated entry function\n",
      "#ifdef __cplusplus\n",
      "extern \"C\"\n",
      "#endif\n",
      "TVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n",
      "  return tvmgen_default_fused_nn_conv2d_nn_bias_add_nn_relu(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lib.get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: InferType: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Running pass AnnotateTargetFunc\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: InferType: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: AnnotateTargetFunc: Executing function pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: AnnotateTargetFunc: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = nn.conv2d(%d1, %w1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: AnnotateTargetFunc: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = annotation.compiler_begin(%d1, compiler=\"dnnl\") /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = annotation.compiler_begin(%w1, compiler=\"dnnl\") /* ty=Tensor[(32, 32, 3, 3), float32] */;\n",
      "  %2 = nn.conv2d(%0, %1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %3 = annotation.compiler_end(%2, compiler=\"dnnl\") /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4 = annotation.compiler_begin(%3, compiler=\"default\") /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %5 = annotation.compiler_begin(%b1, compiler=\"default\") /* ty=Tensor[(32), float32] */;\n",
      "  %6 = nn.bias_add(%4, %5) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %7 = annotation.compiler_end(%6, compiler=\"default\") /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %8 = annotation.compiler_begin(%7, compiler=\"dnnl\") /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %9 = nn.relu(%8) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  annotation.compiler_end(%9, compiler=\"dnnl\") /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: AnnotateTargetFunc: InferType: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Running pass InferType\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: InferType: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Running pass FlattenNestedTuples\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: FlattenNestedTuples: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: FlattenNestedTuples: InferType: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Running pass RemoveDefaultAnnotations\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: RemoveDefaultAnnotations: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: RemoveDefaultAnnotations: InferType: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Running pass PartitionGraph\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: PartitionGraph: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: PartitionGraph: InferType: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: PartitionGraph: InferType: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: PartitionGraph: InferType: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Running pass NameMangleExtFuncs\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: NameMangleExtFuncs: Executing module pass with opt level: 0\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Running pass InferType\n",
      "[12:12:33] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: InferType: Executing module pass with opt level: 0\n"
     ]
    }
   ],
   "source": [
    "if not tvm.get_global_func(\"relay.ext.dnnl\", True):\n",
    "    print(\"skip because DNNL codegen is not available\")\n",
    "\n",
    "mod = get_demo_mod()\n",
    "mod = relay.transform.AnnotateTarget(\"dnnl\")(mod)\n",
    "mod = relay.transform.PartitionGraph()(mod)\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass RemoveUnusedFunctions\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: RemoveUnusedFunctions: Executing module pass with opt level: 1\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass ToBasicBlockNormalForm\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: ToBasicBlockNormalForm: Executing module pass with opt level: 1\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass qnn.Legalize\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass QnnLegalize\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: QnnLegalize: Executing function pass with opt level: 1\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: QnnLegalize: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: QnnLegalize: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: QnnLegalize: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass QnnCanonicalize\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: QnnCanonicalize: Executing function pass with opt level: 1\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: QnnCanonicalize: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: QnnCanonicalize: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: QnnCanonicalize: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass Legalize\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: Legalize: Executing function pass with opt level: 1\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: Legalize: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: Legalize: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: Legalize: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass SimplifyInference\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: SimplifyInference: Executing function pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: SimplifyInference: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: SimplifyInference: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyInference: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass EliminateCommonSubexpr\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'EliminateCommonSubexpr'\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass CombineParallelConv2d\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'CombineParallelConv2d'\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass CombineParallelDense\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'CombineParallelDense'\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass CombineParallelBatchMatmul\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'CombineParallelBatchMatmul'\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FoldConstant\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: FoldConstant: Executing function pass with opt level: 2\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: FoldConstant: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: FoldConstant: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: FoldConstant: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FoldScaleAxis\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass BackwardFoldScaleAxis\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'BackwardFoldScaleAxis'\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass ForwardFoldScaleAxis\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'ForwardFoldScaleAxis'\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FoldConstant\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: FoldConstant: Executing function pass with opt level: 2\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: FoldConstant: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: FoldConstant: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: FoldConstant: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass SimplifyExpr\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: SimplifyExpr: Executing function pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: SimplifyExpr: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: SimplifyExpr: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExpr: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass CanonicalizeCast\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'CanonicalizeCast'\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass CanonicalizeOps\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'CanonicalizeOps'\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FlattenAtrousConv\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: FlattenAtrousConv: Executing function pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: FlattenAtrousConv: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: FlattenAtrousConv: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: FlattenAtrousConv: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass InferType\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass AlterOpLayout\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'AlterOpLayout'\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass SimplifyExprPostAlterOp\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: SimplifyExprPostAlterOp: Executing function pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: SimplifyExprPostAlterOp: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: PatternRewriter: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: SimplifyExprPostAlterOp: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SimplifyExprPostAlterOp: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FastMath\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:447: Build: skipping disabled pass 'FastMath'\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FoldConstant\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: FoldConstant: Executing function pass with opt level: 2\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: FoldConstant: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: FoldConstant: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: FoldConstant: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass SplitArgs\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: SplitArgs: Executing function pass with opt level: 1\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: SplitArgs: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: SplitArgs: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0]) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: SplitArgs: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass PlanDevices\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass PlanDevicesRewrite\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: PlanDevicesRewrite: Executing function pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: PlanDevicesRewrite: Input module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0]) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: PlanDevicesRewrite: Output module:\n",
      "def @main(%d1: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0]) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: PlanDevicesRewrite: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass PlanDevicesCore\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: PlanDevicesCore: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass FuseOps\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: FuseOps: Executing function pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: FuseOps: Input module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = nn.bias_add(%0, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  @tvmgen_default_dnnl_main_2(%1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: FuseOps: Output module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = @tvmgen_default_dnnl_main_0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = fn (%p0: Tensor[(1, 32, 56, 56), float32], %p1: Tensor[(32), float32], Primitive=1) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.bias_add(%p0, %p1)\n",
      "  };\n",
      "  %2 = %1(%0, %b1);\n",
      "  @tvmgen_default_dnnl_main_2(%2) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_0(%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "\n",
      "def @tvmgen_default_dnnl_main_2(%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: FuseOps: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InlineGlobals: Executing module pass with opt level: 1\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: LabelOps: Executing function pass with opt level: 1\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: LabelOps: Input module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = fn (%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = %0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %3 = %2(%1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4 = fn (%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4(%3) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: LabelOps: Output module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"4315367afdd8921e\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = fn (%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Inline=1, Primitive=1, kernel_layout=\"OIHW\", hash=\"70f5a0cfad8c8261\", data_layout=\"NCHW\", out_layout=\"\", global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = %0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1, hash=\"c790f9e4b4bdc477\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %3 = %2(%1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4 = fn (%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, hash=\"726fe6fee283f8ec\", Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4(%3) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: LabelOps: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: AnnotateMemoryScope: Executing function pass with opt level: 2\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: AnnotateMemoryScope: Input module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"4315367afdd8921e\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = fn (%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Inline=1, Primitive=1, kernel_layout=\"OIHW\", hash=\"70f5a0cfad8c8261\", data_layout=\"NCHW\", out_layout=\"\", global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = %0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1, hash=\"c790f9e4b4bdc477\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %3 = %2(%1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4 = fn (%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, hash=\"726fe6fee283f8ec\", Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4(%3) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: AnnotateMemoryScope: Output module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"4315367afdd8921e\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = fn (%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Inline=1, Primitive=1, kernel_layout=\"OIHW\", hash=\"70f5a0cfad8c8261\", data_layout=\"NCHW\", out_layout=\"\", global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = %0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1, hash=\"c790f9e4b4bdc477\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %3 = %2(%1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4 = fn (%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, hash=\"726fe6fee283f8ec\", Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4(%3) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'runtime' = cpp\n",
      "}\n",
      "\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: AnnotateMemoryScope: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: Running pass RelayToTIRTargetHook\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: GraphExecutorCodegen: RelayToTIRTargetHook: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: Running pass LowerTE\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: GraphExecutorCodegen: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: GraphExecutorCodegen: LowerTE: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:124: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Executing function pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:125: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Input module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"4315367afdd8921e\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = fn (%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Inline=1, Primitive=1, kernel_layout=\"OIHW\", hash=\"70f5a0cfad8c8261\", data_layout=\"NCHW\", out_layout=\"\", global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = %0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1, hash=\"c790f9e4b4bdc477\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %3 = %2(%1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4 = fn (%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, hash=\"726fe6fee283f8ec\", Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4(%3) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "}\n",
      "attributes {\n",
      "  'constant_memory_pools' = (nullptr)\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'main_func_info' = FunctionInfoNode(\n",
      "workspace_sizes={c -keys=cpu : 802816},\n",
      "  io_sizes={c -keys=cpu : 839808},\n",
      "  constant_sizes={c -keys=cpu : 0},\n",
      "  tir_primfuncs={},\n",
      "  relay_primfuncs={c -keys=cpu : fn (%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"4315367afdd8921e\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = fn (%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Inline=1, Primitive=1, kernel_layout=\"OIHW\", hash=\"70f5a0cfad8c8261\", data_layout=\"NCHW\", out_layout=\"\", global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = %0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1, hash=\"c790f9e4b4bdc477\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %3 = %2(%1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4 = fn (%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, hash=\"726fe6fee283f8ec\", Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4(%3) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "} /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */\n",
      "})\n",
      "  'runtime' = cpp\n",
      "  'workspace_memory_pools' = (nullptr)\n",
      "}\n",
      "\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectPrefetch\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.TextureFlatten\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlatten\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferShapeLegalize\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferStrideLegalize\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ThreadScopePropagate\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BufferBindUnwrapper\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ApplyLayoutTransforms\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageFlattener\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.AssertSimplifier\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerCrossThreadReduction\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerInitBlock\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.PlanAndUpdateBufferAllocationLocation\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ConvertBlocksToOpaque\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnifyThreadBinding\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.ManifestSharedMemoryLocalStage\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CompactBufferAllocation\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerAutoCopy\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerMatchBuffer\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectSoftwarePipeline\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LowerOpaqueBlock\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.FlattenBuffer\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.BF16ComputeLegalize\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.NarrowDataType\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.LoopPartition\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.VectorizeLoop\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectVirtualThread\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InjectDoubleBuffer\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.StorageRewrite\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.UnrollLoop\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RenormalizeSplitPattern\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RewriteUnsafeSelect\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.HoistIfThenElse\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.InsertHoistIfThenElse\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.Simplify\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.RemoveNoOp\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Running pass tir.CommonSubexprElimTIR\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/relay/ir/transform.cc:148: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: Output module:\n",
      "def @main(%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"4315367afdd8921e\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = (%d1, %w1);\n",
      "  %1 = call_lowered(@tvmgen_default_dnnl_main_0, %0, metadata={\"relay_attrs\"={__dict__={\"Compiler\"=\"dnnl\", \"Inline\"=1, \"Primitive\"=1, \"kernel_layout\"=\"OIHW\", \"hash\"=\"70f5a0cfad8c8261\", \"data_layout\"=\"NCHW\", \"out_layout\"=\"\", \"global_symbol\"=\"tvmgen_default_dnnl_main_0\"}}, \"all_prim_fn_vars\"=[]});\n",
      "  %2 = (%1, %b1);\n",
      "  %3 = call_lowered(@tvmgen_default_fused_nn_bias_add, %2, metadata={\"relay_attrs\"={__dict__={\"Primitive\"=1, \"hash\"=\"c790f9e4b4bdc477\"}}, \"all_prim_fn_vars\"=['tvmgen_default_fused_nn_bias_add']});\n",
      "  %4 = (%3,);\n",
      "  call_lowered(@tvmgen_default_dnnl_main_2, %4, metadata={\"relay_attrs\"={__dict__={\"Compiler\"=\"dnnl\", \"Primitive\"=1, \"hash\"=\"726fe6fee283f8ec\", \"Inline\"=1, \"global_symbol\"=\"tvmgen_default_dnnl_main_2\"}}, \"all_prim_fn_vars\"=[]})\n",
      "}\n",
      "attributes {\n",
      "  'constant_memory_pools' = (nullptr)\n",
      "  'executor' = graph{\"link-params\": T.bool(False)}\n",
      "  'main_func_info' = FunctionInfoNode(\n",
      "workspace_sizes={c -keys=cpu : 802816},\n",
      "  io_sizes={c -keys=cpu : 839808},\n",
      "  constant_sizes={c -keys=cpu : 0},\n",
      "  tir_primfuncs={},\n",
      "  relay_primfuncs={c -keys=cpu : fn (%d1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %w1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, %b1 {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))}: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, runtime=meta[Runtime][0], hash=\"4315367afdd8921e\", kernel_layout=\"OIHW\", executor=meta[Executor][0], data_layout=\"NCHW\", out_layout=\"\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=4098bc0, kind='c', keys={'cpu'}, host=Target(id=3bfd280, kind='c', keys={'cpu'})))) -> Tensor[(1, 32, 56, 56), float32] {\n",
      "  %0 = fn (%dnnl_0_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %dnnl_0_i1: Tensor[(32, 32, 3, 3), float32] /* ty=Tensor[(32, 32, 3, 3), float32] */, Compiler=\"dnnl\", Inline=1, Primitive=1, kernel_layout=\"OIHW\", hash=\"70f5a0cfad8c8261\", data_layout=\"NCHW\", out_layout=\"\", global_symbol=\"tvmgen_default_dnnl_main_0\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.conv2d(%dnnl_0_i0, %dnnl_0_i1, padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %1 = %0(%d1, %w1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %2 = fn (%p0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, %p1: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1, hash=\"c790f9e4b4bdc477\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.bias_add(%p0, %p1) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %3 = %2(%1, %b1) /* ty=Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4 = fn (%dnnl_2_i0: Tensor[(1, 32, 56, 56), float32] /* ty=Tensor[(1, 32, 56, 56), float32] */, Compiler=\"dnnl\", Primitive=1, hash=\"726fe6fee283f8ec\", Inline=1, global_symbol=\"tvmgen_default_dnnl_main_2\") -> Tensor[(1, 32, 56, 56), float32] {\n",
      "    nn.relu(%dnnl_2_i0) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "  } /* ty=fn (Tensor[(1, 32, 56, 56), float32]) -> Tensor[(1, 32, 56, 56), float32] */;\n",
      "  %4(%3) /* ty=Tensor[(1, 32, 56, 56), float32] */\n",
      "} /* ty=fn (Tensor[(1, 32, 56, 56), float32], Tensor[(32, 32, 3, 3), float32], Tensor[(32), float32]) -> Tensor[(1, 32, 56, 56), float32] */\n",
      "})\n",
      "  'runtime' = cpp\n",
      "  'workspace_memory_pools' = (nullptr)\n",
      "}\n",
      "\n",
      "\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: GraphExecutorCodegen: LowerTE: LowerTensorExpr: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: Running pass InferType\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: GraphExecutorCodegen: InferType: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: GraphExecutorCodegen: Running pass tir.ExtractPrimFuncConstants\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: GraphExecutorCodegen: tir.ExtractPrimFuncConstants: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.calculate_allocated_bytes\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: tir.calculate_allocated_bytes: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerVtcmAlloc\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.BindTarget\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.VerifyMemory\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: tir.VerifyMemory: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.AnnotateEntryFunc\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.ThreadSync\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.ThreadSync\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.MergeDynamicSharedMemoryAllocations\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.ThreadSync\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.InferFragment\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerThreadAllreduce\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.MakePackedAPI\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: tir.MakePackedAPI: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.BF16StorageLegalize\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.SplitHostDevice\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: tir.SplitHostDevice: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:382: Build: tir.SplitHostDevice: tir.ConvertSSA: Executing module pass with opt level: 0\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.Filter\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.BindTarget\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerTVMBuiltin\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerCustomDatatypes\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerIntrin\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerDeviceStorageAccessInfo\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.CombineContextCall\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.Filter\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.BindTarget\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerWarpMemory\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.Simplify\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerCustomDatatypes\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerDeviceStorageAccessInfo\n",
      "[12:12:53] /home/qzylalala/work_space/tvm/src/ir/transform.cc:443: Build: Running pass tir.LowerIntrin\n",
      "/tmp/ipykernel_62638/1202328826.py:2: DeprecationWarning: legacy graph executor behavior of producing json / lib / params will be removed in the next release. Please see documents of tvm.contrib.graph_executor.GraphModule for the  new recommended usage.\n",
      "  graph, lib, params = relay.build(mod, target=\"c\", params=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// tvm target: c -keys=cpu \n",
      "#define TVM_EXPORTS\n",
      "#include \"tvm/runtime/c_runtime_api.h\"\n",
      "#include \"tvm/runtime/c_backend_api.h\"\n",
      "#include <math.h>\n",
      "#include <stdbool.h>\n",
      "#ifdef __cplusplus\n",
      "extern \"C\"\n",
      "#endif\n",
      "TVM_DLL int32_t tvmgen_default_fused_nn_bias_add(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n",
      "  void* arg_p0 = (((TVMValue*)args)[0].v_handle);\n",
      "  int32_t arg_p0_code = arg_type_ids[0];\n",
      "  void* arg_p1 = (((TVMValue*)args)[1].v_handle);\n",
      "  int32_t arg_p1_code = arg_type_ids[1];\n",
      "  void* arg_T_add = (((TVMValue*)args)[2].v_handle);\n",
      "  int32_t arg_T_add_code = arg_type_ids[2];\n",
      "  void* p0 = (((DLTensor*)arg_p0)[0].data);\n",
      "  void* arg_p0_shape = (((DLTensor*)arg_p0)[0].shape);\n",
      "  void* arg_p0_strides = (((DLTensor*)arg_p0)[0].strides);\n",
      "  int32_t dev_id = (((DLTensor*)arg_p0)[0].device.device_id);\n",
      "  void* p1 = (((DLTensor*)arg_p1)[0].data);\n",
      "  void* arg_p1_shape = (((DLTensor*)arg_p1)[0].shape);\n",
      "  void* arg_p1_strides = (((DLTensor*)arg_p1)[0].strides);\n",
      "  void* T_add = (((DLTensor*)arg_T_add)[0].data);\n",
      "  void* arg_T_add_shape = (((DLTensor*)arg_T_add)[0].shape);\n",
      "  void* arg_T_add_strides = (((DLTensor*)arg_T_add)[0].strides);\n",
      "  if (!(arg_p0_strides == NULL)) {\n",
      "  }\n",
      "  if (!(arg_p1_strides == NULL)) {\n",
      "  }\n",
      "  if (!(arg_T_add_strides == NULL)) {\n",
      "  }\n",
      "  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 32; ++ax0_ax1_fused) {\n",
      "    for (int32_t ax2 = 0; ax2 < 56; ++ax2) {\n",
      "      for (int32_t ax3_outer = 0; ax3_outer < 4; ++ax3_outer) {\n",
      "        for (int32_t ax3_inner_s = 0; ax3_inner_s < 16; ++ax3_inner_s) {\n",
      "          if (((ax3_outer * 2) + (ax3_inner_s >> 3)) < 7) {\n",
      "            int32_t cse_var_1 = ((((ax0_ax1_fused * 3136) + (ax2 * 56)) + (ax3_outer * 16)) + ax3_inner_s);\n",
      "            ((float*)T_add)[cse_var_1] = (((float*)p0)[cse_var_1] + ((float*)p1)[ax0_ax1_fused]);\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "// CodegenC: NOTE: Auto-generated entry function\n",
      "#ifdef __cplusplus\n",
      "extern \"C\"\n",
      "#endif\n",
      "TVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n",
      "  return tvmgen_default_fused_nn_bias_add(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tvm.transform.PassContext(opt_level=2):\n",
    "    graph, lib, params = relay.build(mod, target=\"c\", params=None)\n",
    "\n",
    "lib.export_library(\"liba.so\")    \n",
    "print(lib.imported_modules[0].get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"symbol\": \"dnnl_tvmgen_default_dnnl_main_2\", \n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"op\": \"input\", \n",
      "      \"name\": \"dnnl_2_i0\", \n",
      "      \"attrs\": {\n",
      "        \"dtype\": [\n",
      "          [\n",
      "            \"float32\"\n",
      "          ]\n",
      "        ], \n",
      "        \"shape\": [\n",
      "          [\n",
      "            [1, 32, 56, 56]\n",
      "          ]\n",
      "        ]\n",
      "      }\n",
      "    }, \n",
      "    {\n",
      "      \"op\": \"kernel\", \n",
      "      \"name\": \"nn.relu\", \n",
      "      \"inputs\": [[\n",
      "          0, \n",
      "          0, \n",
      "          0]], \n",
      "      \"attrs\": {\n",
      "        \"dtype\": [\n",
      "          [\n",
      "            \"float32\"\n",
      "          ]\n",
      "        ], \n",
      "        \"num_outputs\": \"1\", \n",
      "        \"num_inputs\": \"1\", \n",
      "        \"shape\": [\n",
      "          [\n",
      "            [1, 32, 56, 56]\n",
      "          ]\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ], \n",
      "  \"arg_nodes\": [0], \n",
      "  \"heads\": [[\n",
      "      1, \n",
      "      0, \n",
      "      0]], \n",
      "  \"node_row_ptr\": [0, 1, 2]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(lib.imported_modules[1].get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String dump of section '.rodata':\n",
      "  [    18]  const_loader\n",
      "  [    44]  tvmgen_default_dnnl_main_0^Z\n",
      "  [    66]  tvmgen_default_dnnl_main_2^B\n",
      "  [    a0]  dnnl_json^Z\n",
      "  [    b1]  tvmgen_default_dnnl_main_0|^G\n",
      "  [    d3]  {^J  \"symbol\": \"dnnl_tvmgen_default_dnnl_main_0\", ^J  \"nodes\": [^J    {^J      \"op\": \"input\", ^J      \"name\": \"dnnl_0_i0\", ^J      \"attrs\": {^J        \"dtype\": [^J          [^J            \"float32\"^J          ]^J        ], ^J        \"shape\": [^J          [^J            [1, 32, 56, 56]^J          ]^J        ]^J      }^J    }, ^J    {^J      \"op\": \"input\", ^J      \"name\": \"dnnl_0_i1\", ^J      \"attrs\": {^J        \"dtype\": [^J          [^J            \"float32\"^J          ]^J        ], ^J        \"shape\": [^J          [^J            [32, 32, 3, 3]^J          ]^J        ]^J      }^J    }, ^J    {^J      \"op\": \"kernel\", ^J      \"name\": \"nn.conv2d\", ^J      \"inputs\": [[^J          0, ^J          0, ^J          0], [^J          1, ^J          0, ^J          0]], ^J      \"attrs\": {^J        \"strides\": [^J          [^J            \"1\", ^J            \"1\"^J          ]^J        ], ^J        \"num_outputs\": \"1\", ^J        \"padding\": [^J          [^J            \"1\", ^J            \"1\", ^J            \"1\", ^J            \"1\"^J          ]^J        ], ^J        \"dilation\": [^J          [^J            \"1\", ^J            \"1\"^J          ]^J        ], ^J        \"out_dtype\": [^J          [^J            \"\"^J          ]^J        ], ^J        \"groups\": [^J          [^J            \"1\"^J          ]^J        ], ^J        \"channels\": [^J          [^J            \"\"^J          ]^J        ], ^J        \"shape\": [^J          [^J            [1, 32, 56, 56]^J          ]^J        ], ^J        \"num_inputs\": \"2\", ^J        \"data_layout\": [^J          [^J            \"NCHW\"^J          ]^J        ], ^J        \"kernel_layout\": [^J          [^J            \"OIHW\"^J          ]^J        ], ^J        \"out_layout\": [^J          [^J            \"\"^J          ]^J        ], ^J        \"kernel_size\": [^J          [^J            \"\"^J          ]^J        ], ^J        \"dtype\": [^J          [^J            \"float32\"^J          ]^J        ]^J      }^J    }^J  ], ^J  \"arg_nodes\": [0, 1], ^J  \"heads\": [[^J      2, ^J      0, ^J      0]], ^J  \"node_row_ptr\": [0, 1, 2, 3]^J}\n",
      "  [   85f]  dnnl_json^Z\n",
      "  [   870]  tvmgen_default_dnnl_main_2^H^C\n",
      "  [   892]  {^J  \"symbol\": \"dnnl_tvmgen_default_dnnl_main_2\", ^J  \"nodes\": [^J    {^J      \"op\": \"input\", ^J      \"name\": \"dnnl_2_i0\", ^J      \"attrs\": {^J        \"dtype\": [^J          [^J            \"float32\"^J          ]^J        ], ^J        \"shape\": [^J          [^J            [1, 32, 56, 56]^J          ]^J        ]^J      }^J    }, ^J    {^J      \"op\": \"kernel\", ^J      \"name\": \"nn.relu\", ^J      \"inputs\": [[^J          0, ^J          0, ^J          0]], ^J      \"attrs\": {^J        \"dtype\": [^J          [^J            \"float32\"^J          ]^J        ], ^J        \"num_outputs\": \"1\", ^J        \"num_inputs\": \"1\", ^J        \"shape\": [^J          [^J            [1, 32, 56, 56]^J          ]^J        ]^J      }^J    }^J  ], ^J  \"arg_nodes\": [0], ^J  \"heads\": [[^J      1, ^J      0, ^J      0]], ^J  \"node_row_ptr\": [0, 1, 2]^J}\n",
      "  [   baa]  _lib^L\n",
      "  [   bb6]  _import_tree^E\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!readelf -p .rodata liba.so"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于生成的 lib 中只能看到 dnnl 相应的 json 数据, 但找不到使用这段 json 的代码, 所以 lib 本身并非 self-contained, 需要 runtime 里也需要有 dnnl 对应的代码"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
