{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qzylalala/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import tvm\n",
    "from tvm import te\n",
    "import tvm.testing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import ir as I\n",
      "# from tvm.script import tir as T\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def main(A: T.Buffer((1024, 64), \"float32\"), B: T.Buffer((512, 64), \"float32\"), C: T.Buffer((1024, 512), \"float32\")):\n",
      "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
      "        for i, j in T.grid(1024, 512):\n",
      "            C_1 = T.Buffer((524288,), data=C.data)\n",
      "            C_1[i * 512 + j] = T.float32(0)\n",
      "            for k in range(64):\n",
      "                cse_var_1: T.int32 = i * 512 + j\n",
      "                A_1 = T.Buffer((65536,), data=A.data)\n",
      "                B_1 = T.Buffer((32768,), data=B.data)\n",
      "                C_1[cse_var_1] = C_1[cse_var_1] + A_1[i * 64 + k] * B_1[j * 64 + k]\n"
     ]
    }
   ],
   "source": [
    "N, M, L = 1024, 512, 64\n",
    "A = te.placeholder((N, L), name=\"A\")\n",
    "B = te.placeholder((M, L), name=\"B\")\n",
    "k = te.reduce_axis((0, L), name=\"k\")\n",
    "C = te.compute((N, M), lambda i, j: te.sum(A[i, k] * B[j, k], axis=k), name=\"C\")\n",
    "s = te.create_schedule(C.op)\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import ir as I\n",
      "# from tvm.script import tir as T\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def main(A: T.Buffer((1024, 64), \"float32\"), B: T.Buffer((512, 64), \"float32\"), C: T.Buffer((1024, 512), \"float32\")):\n",
      "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
      "        for i, j_outer, j_inner in T.grid(1024, 32, 16):\n",
      "            C_1 = T.Buffer((524288,), data=C.data)\n",
      "            C_1[i * 512 + j_outer * 16 + j_inner] = T.float32(0)\n",
      "            for k in range(64):\n",
      "                cse_var_1: T.int32 = i * 512 + j_outer * 16 + j_inner\n",
      "                A_1 = T.Buffer((65536,), data=A.data)\n",
      "                B_1 = T.Buffer((32768,), data=B.data)\n",
      "                C_1[cse_var_1] = C_1[cse_var_1] + A_1[i * 64 + k] * B_1[j_outer * 1024 + j_inner * 64 + k]\n"
     ]
    }
   ],
   "source": [
    "factor = 16\n",
    "x, y = C.op.axis\n",
    "(z,) = C.op.reduce_axis\n",
    "yo, yi = s[C].split(y, factor=factor)\n",
    "s[C].reorder(x, yo, yi, z)\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrin_gemv(m, l):\n",
    "    a = te.placeholder((l,), name=\"a\")\n",
    "    b = te.placeholder((m, l), name=\"b\")\n",
    "    k = te.reduce_axis((0, l), name=\"k\")\n",
    "    c = te.compute((m,), lambda i: te.sum(a[k] * b[i, k], axis=k), name=\"c\")\n",
    "    Ab = tvm.tir.decl_buffer(a.shape, a.dtype, name=\"A\", offset_factor=1, strides=[1])\n",
    "    Bb = tvm.tir.decl_buffer(b.shape, b.dtype, name=\"B\", offset_factor=1, strides=[te.var(\"s1\"), 1])\n",
    "    Cb = tvm.tir.decl_buffer(c.shape, c.dtype, name=\"C\", offset_factor=1, strides=[1])\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        ib = tvm.tir.ir_builder.create()\n",
    "        aa, bb = ins\n",
    "        cc = outs[0]\n",
    "        ib.emit(\n",
    "            tvm.tir.call_extern(\n",
    "                \"int32\",\n",
    "                \"gemv_update\",\n",
    "                cc.access_ptr(\"w\"),\n",
    "                aa.access_ptr(\"r\"),\n",
    "                bb.access_ptr(\"r\"),\n",
    "                m,\n",
    "                l,\n",
    "                bb.strides[0],\n",
    "            )\n",
    "        )\n",
    "        return ib.get()\n",
    "\n",
    "    return te.decl_tensor_intrin(c.op, intrin_func, binds={a: Ab, b: Bb, c: Cb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import ir as I\n",
      "# from tvm.script import tir as T\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def main(A: T.Buffer((1024, 64), \"float32\"), B: T.Buffer((512, 64), \"float32\"), C: T.Buffer((1024, 512), \"float32\")):\n",
      "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
      "        for i, j_outer in T.grid(1024, 32):\n",
      "            T.call_extern(\"int32\", \"gemv_update\", T.tvm_access_ptr(T.type_annotation(\"float32\"), C.data, i * 512 + j_outer * 16, 16, 2), T.tvm_access_ptr(T.type_annotation(\"float32\"), A.data, i * 64, 64, 1), T.tvm_access_ptr(T.type_annotation(\"float32\"), B.data, j_outer * 1024, 1024, 1), 16, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "gemv = intrin_gemv(factor, L)\n",
    "s[C].tensorize(yi, gemv)\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemv_impl():\n",
    "    cc_code = \"\"\"\n",
    "      extern \"C\" int gemv_update(float *cc, float *aa, float *bb, int m, int l, int stride) {\n",
    "        for (int i = 0; i < m; ++i) {\n",
    "            for (int j = 0; j < l; ++j) {\n",
    "                cc[i] += aa[j] * bb[i * stride + j];\n",
    "            }\n",
    "        }\n",
    "        return 0;\n",
    "      }\n",
    "    \"\"\"\n",
    "    from tvm.contrib import utils, clang\n",
    "\n",
    "    temp = utils.tempdir()\n",
    "    ll_path = temp.relpath(\"temp.ll\")\n",
    "    # 从 C 源代码创建 LLVM ir\n",
    "    ll_code = clang.create_llvm(cc_code, output=ll_path)\n",
    "    return ll_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import ir as I\n",
      "# from tvm.script import tir as T\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def main(A: T.Buffer((1024, 64), \"float32\"), B: T.Buffer((512, 64), \"float32\"), C: T.Buffer((1024, 512), \"float32\")):\n",
      "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
      "        i = T.int32()\n",
      "        T.attr(T.iter_var(i, None, \"DataPar\", \"\"), \"pragma_import_llvm\", metadata[\"tir.StringImm\"][0])\n",
      "        for i, j_outer in T.grid(1024, 32):\n",
      "            T.call_extern(\"int32\", \"gemv_update\", T.tvm_access_ptr(T.type_annotation(\"float32\"), C.data, i * 512 + j_outer * 16, 16, 2), T.tvm_access_ptr(T.type_annotation(\"float32\"), A.data, i * 64, 64, 1), T.tvm_access_ptr(T.type_annotation(\"float32\"), B.data, j_outer * 1024, 1024, 1), 16, 64, 64)\n",
      "\n",
      "# Metadata omitted. Use show_meta=True in script() method to show it.\n"
     ]
    }
   ],
   "source": [
    "s[C].pragma(x, \"import_llvm\", gemv_impl())\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = tvm.build(s, [A, B, C], target=\"llvm\", name=\"gemv\")\n",
    "\n",
    "from tvm.topi.utils import get_const_tuple\n",
    "\n",
    "dtype = A.dtype\n",
    "dev = tvm.device(\"cpu\", 0)\n",
    "a = np.random.uniform(size=get_const_tuple(A.shape)).astype(dtype)\n",
    "b = np.random.uniform(size=get_const_tuple(B.shape)).astype(dtype)\n",
    "c = tvm.nd.array(np.zeros(get_const_tuple(C.shape), dtype=dtype), dev)\n",
    "func(tvm.nd.array(a, dev), tvm.nd.array(b, dev), c)\n",
    "tvm.testing.assert_allclose(c.numpy(), np.dot(a, b.T), rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zo, zi = s[C].split(z, factor=factor)\n",
    "s[C].reorder(x, yo, zo, yi, zi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemv_impl():\n",
    "    cc_code = \"\"\"\n",
    "      extern \"C\" int gemv_update(float *cc, float *aa, float *bb, int m, int l, int stride) {\n",
    "        for (int i = 0; i < m; ++i) {\n",
    "            for (int j = 0; j < l; ++j) {\n",
    "                cc[i] += aa[j] * bb[i * stride + j];\n",
    "            }\n",
    "        }\n",
    "        return 0;\n",
    "      }\n",
    "      extern \"C\" int gemv_reset(float *cc, int m) {\n",
    "        for (int i = 0; i < m; ++i) {\n",
    "            cc[i] = 0.0;\n",
    "        }\n",
    "        return 0;\n",
    "      }\n",
    "    \"\"\"\n",
    "    from tvm.contrib import utils, clang\n",
    "\n",
    "    temp = utils.tempdir()\n",
    "    ll_path = temp.relpath(\"temp.ll\")\n",
    "    # 从 C 源代码创建 LLVM ir\n",
    "    ll_code = clang.create_llvm(cc_code, output=ll_path)\n",
    "    return ll_code\n",
    "\n",
    "def intrin_gemv(m, l):\n",
    "    a = te.placeholder((l,), name=\"a\")\n",
    "    b = te.placeholder((m, l), name=\"b\")\n",
    "    k = te.reduce_axis((0, l), name=\"k\")\n",
    "    c = te.compute((m,), lambda i: te.sum(a[k] * b[i, k], axis=k), name=\"c\")\n",
    "    Ab = tvm.tir.decl_buffer(a.shape, a.dtype, name=\"A\", offset_factor=1, strides=[1])\n",
    "    Bb = tvm.tir.decl_buffer(b.shape, b.dtype, name=\"B\", offset_factor=1, strides=[te.var(\"s1\"), 1])\n",
    "    Cb = tvm.tir.decl_buffer(c.shape, c.dtype, name=\"C\", offset_factor=1, strides=[1])\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        aa, bb = ins\n",
    "        cc = outs[0]\n",
    "\n",
    "        def _body():\n",
    "            ib = tvm.tir.ir_builder.create()\n",
    "            ib.emit(\n",
    "                tvm.tir.call_extern(\n",
    "                    \"int32\",\n",
    "                    \"gemv_update\",\n",
    "                    cc.access_ptr(\"w\"),\n",
    "                    aa.access_ptr(\"r\"),\n",
    "                    bb.access_ptr(\"r\"),\n",
    "                    m,\n",
    "                    l,\n",
    "                    bb.strides[0],\n",
    "                )\n",
    "            )\n",
    "            return ib.get()\n",
    "\n",
    "        def _reduce_reset():\n",
    "            ib = tvm.tir.ir_builder.create()\n",
    "            ib.emit(tvm.tir.call_extern(\"int32\", \"gemv_reset\", cc.access_ptr(\"w\"), m))\n",
    "            return ib.get()\n",
    "\n",
    "        def _reduce_update():\n",
    "            return _body()\n",
    "\n",
    "        return _body(), _reduce_reset(), _reduce_update()\n",
    "\n",
    "    return te.decl_tensor_intrin(c.op, intrin_func, binds={a: Ab, b: Bb, c: Cb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemv = intrin_gemv(factor, factor)\n",
    "s[C].tensorize(yi, gemv)\n",
    "s[C].pragma(yo, \"import_llvm\", gemv_impl())\n",
    "\n",
    "func = tvm.build(s, [A, B, C], target=\"llvm\", name=\"gemv\")\n",
    "a = np.random.uniform(size=get_const_tuple(A.shape)).astype(dtype)\n",
    "b = np.random.uniform(size=get_const_tuple(B.shape)).astype(dtype)\n",
    "c = tvm.nd.array(np.zeros(get_const_tuple(C.shape), dtype=dtype), dev)\n",
    "func(tvm.nd.array(a, dev), tvm.nd.array(b, dev), c)\n",
    "tvm.testing.assert_allclose(c.numpy(), np.dot(a, b.T), rtol=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
